{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hMBdgi9K9Q6p"
   },
   "source": [
    "Natural Language Processing (NLP) is a unique subset of Machine Learning which cares about the real life unstructured data. Although computers cannot identify and process the string inputs, the libraries like NLTK, TextBlob and many others found a way to process string mathematically. Twitter is a platform where most of the people express their feelings towards the current context. In this notebook, we will fetch the data of some individuals from twitter and analyze it to the ability of the indviduals to lead others in many dimensions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ApGG-mnBBpQr"
   },
   "source": [
    "First we will import the libraries that we will be using in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UI2fIQFxrNLB",
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "from datetime import datetime, date, time, timedelta\n",
    "from collections import Counter\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import API\n",
    "from tweepy import Cursor\n",
    "# to view all columns\n",
    "pd.set_option(\"display.max.columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3pf5Xapqrq0M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in /Users/DClinton/opt/anaconda3/lib/python3.7/site-packages (3.8.0)\n",
      "Requirement already satisfied: PySocks>=1.5.7 in /Users/DClinton/opt/anaconda3/lib/python3.7/site-packages (from tweepy) (1.7.1)\n",
      "Requirement already satisfied: requests>=2.11.1 in /Users/DClinton/opt/anaconda3/lib/python3.7/site-packages (from tweepy) (2.24.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/DClinton/opt/anaconda3/lib/python3.7/site-packages (from tweepy) (1.14.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/DClinton/opt/anaconda3/lib/python3.7/site-packages (from tweepy) (1.3.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/DClinton/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.11.1->tweepy) (1.25.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/DClinton/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.11.1->tweepy) (2.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/DClinton/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.11.1->tweepy) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/DClinton/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.11.1->tweepy) (2019.11.28)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/DClinton/opt/anaconda3/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->tweepy) (3.1.0)\n",
      "Requirement already satisfied: nltk in /Users/DClinton/opt/anaconda3/lib/python3.7/site-packages (3.4.5)\n",
      "Requirement already satisfied: six in /Users/DClinton/opt/anaconda3/lib/python3.7/site-packages (from nltk) (1.14.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/DClinton/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweet-preprocessor in /Users/DClinton/opt/anaconda3/lib/python3.7/site-packages (0.6.0)\r\n"
     ]
    }
   ],
   "source": [
    "#Import the necessary methods from tweepy library  \n",
    "\n",
    "#install tweepy if you don't have it\n",
    "!pip install tweepy\n",
    "import tweepy\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "\n",
    "#sentiment analysis package\n",
    "#!pip install textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "#general text pre-processor\n",
    "!pip install nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "\n",
    "#tweet pre-processor \n",
    "!pip install tweet-preprocessor\n",
    "import preprocessor as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Name</th>\n",
       "      <th>Twittername</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>Jeffrey Gettleman</td>\n",
       "      <td>@gettleman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>99</td>\n",
       "      <td>Africa24 Media</td>\n",
       "      <td>@a24media</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98</td>\n",
       "      <td>Scapegoat</td>\n",
       "      <td>@andiMakinana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>97</td>\n",
       "      <td>Africa Check</td>\n",
       "      <td>@AfricaCheck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>96</td>\n",
       "      <td>James Copnall</td>\n",
       "      <td>@JamesCopnall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5</td>\n",
       "      <td>Julius Sello Malema</td>\n",
       "      <td>@Julius_S_Malema</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>4</td>\n",
       "      <td>News24</td>\n",
       "      <td>@News24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>3</td>\n",
       "      <td>Jacob G. Zuma</td>\n",
       "      <td>@SAPresident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2</td>\n",
       "      <td>Gareth Cliff</td>\n",
       "      <td>@GarethCliff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "      <td>Trevor Noah</td>\n",
       "      <td>@Trevornoah</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Rank                   Name       Twittername\n",
       "0    100     Jeffrey Gettleman         @gettleman\n",
       "1     99        Africa24 Media          @a24media\n",
       "2     98             Scapegoat      @andiMakinana\n",
       "3     97          Africa Check       @AfricaCheck\n",
       "4     96         James Copnall      @JamesCopnall\n",
       "..   ...                    ...               ...\n",
       "95     5   Julius Sello Malema   @Julius_S_Malema\n",
       "96     4                News24            @News24\n",
       "97     3         Jacob G. Zuma       @SAPresident\n",
       "98     2          Gareth Cliff       @GarethCliff\n",
       "99     1           Trevor Noah        @Trevornoah\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('\\Desktop\\export_dataframe.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#striping off the last character in the twittername column\n",
    "data['Twittername'] = data['Twittername'].str.rstrip(')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Name</th>\n",
       "      <th>Twittername</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>Jeffrey Gettleman</td>\n",
       "      <td>@gettleman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>99</td>\n",
       "      <td>Africa24 Media</td>\n",
       "      <td>@a24media</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98</td>\n",
       "      <td>Scapegoat</td>\n",
       "      <td>@andiMakinana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>97</td>\n",
       "      <td>Africa Check</td>\n",
       "      <td>@AfricaCheck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>96</td>\n",
       "      <td>James Copnall</td>\n",
       "      <td>@JamesCopnall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5</td>\n",
       "      <td>Julius Sello Malema</td>\n",
       "      <td>@Julius_S_Malema</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>4</td>\n",
       "      <td>News24</td>\n",
       "      <td>@News24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>3</td>\n",
       "      <td>Jacob G. Zuma</td>\n",
       "      <td>@SAPresident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2</td>\n",
       "      <td>Gareth Cliff</td>\n",
       "      <td>@GarethCliff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "      <td>Trevor Noah</td>\n",
       "      <td>@Trevornoah</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Rank                   Name       Twittername\n",
       "0    100     Jeffrey Gettleman         @gettleman\n",
       "1     99        Africa24 Media          @a24media\n",
       "2     98             Scapegoat      @andiMakinana\n",
       "3     97          Africa Check       @AfricaCheck\n",
       "4     96         James Copnall      @JamesCopnall\n",
       "..   ...                    ...               ...\n",
       "95     5   Julius Sello Malema   @Julius_S_Malema\n",
       "96     4                News24            @News24\n",
       "97     3         Jacob G. Zuma       @SAPresident\n",
       "98     2          Gareth Cliff       @GarethCliff\n",
       "99     1           Trevor Noah        @Trevornoah\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           @gettleman\n",
       "1            @a24media\n",
       "2        @andiMakinana\n",
       "3         @AfricaCheck\n",
       "4        @JamesCopnall\n",
       "            ...       \n",
       "95    @Julius_S_Malema\n",
       "96             @News24\n",
       "97        @SAPresident\n",
       "98        @GarethCliff\n",
       "99         @Trevornoah\n",
       "Name: Twittername, Length: 100, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1=data['Twittername']\n",
    "list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom __future__ import unicode_literals\\nimport tweepy\\nimport time\\nimport unicodecsv as csv\\nimport codecs\\nfrom importlib import reload\\nimport sys \\nwith open('your_file.txt', 'r') as targets_file:\\n    targets_list = targets_file.readlines()\\n\\ntargets_list_filtered = filter(None, targets_list[0].split('\\r'))\\n\\ntargets_list_cleaned = [] \\n\\nfor item in targets_list_filtered:\\n    targets_list_cleaned.append(item.strip('\\n'))\\n\\nwith codecs.open('output.csv', 'w', 'utf-8') as outcsv:\\n    outfile = csv.DictWriter(outcsv, fieldnames=['uID', 'Username', 'Follower Count', 'Verified'])\\n    outfile.writeheader()\\n\\n    for idx, target in enumerate(targets_list_cleaned):\\n        try:\\n            user = api.get_user(target)\\n            outfile.writerow({'uID': target, 'Username': user.name, 'Follower Count': user.followers_count, 'Verified': user.verified})\\n            print(idx, target, user.name, user.followers_count, user.verified)\\n        except tweepy.TweepError as e:\\n            # outfile.writerow(e.message)\\n            print(idx, target, e.message)\\n        \""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from __future__ import unicode_literals\n",
    "import tweepy\n",
    "import time\n",
    "import unicodecsv as csv\n",
    "import codecs\n",
    "from importlib import reload\n",
    "import sys \n",
    "with open('your_file.txt', 'r') as targets_file:\n",
    "    targets_list = targets_file.readlines()\n",
    "\n",
    "targets_list_filtered = filter(None, targets_list[0].split('\\r'))\n",
    "\n",
    "targets_list_cleaned = [] \n",
    "\n",
    "for item in targets_list_filtered:\n",
    "    targets_list_cleaned.append(item.strip('\\n'))\n",
    "\n",
    "with codecs.open('output.csv', 'w', 'utf-8') as outcsv:\n",
    "    outfile = csv.DictWriter(outcsv, fieldnames=['uID', 'Username', 'Follower Count', 'Verified'])\n",
    "    outfile.writeheader()\n",
    "\n",
    "    for idx, target in enumerate(targets_list_cleaned):\n",
    "        try:\n",
    "            user = api.get_user(target)\n",
    "            outfile.writerow({'uID': target, 'Username': user.name, 'Follower Count': user.followers_count, 'Verified': user.verified})\n",
    "            print(idx, target, user.name, user.followers_count, user.verified)\n",
    "        except tweepy.TweepError as e:\n",
    "            # outfile.writerow(e.message)\n",
    "            print(idx, target, e.message)\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import tweepy\\n\\nauth = tweepy.OAuthHandler('hFHw04mGPdEztZe5CMVBTsq9l','cczG6tshnnIFZbdrSNIP7r22NzaFfUsKIC6cRdaglVfP5GyYaN' )\\nauth.set_access_token('990262908696387584-HTgPRm2tySXaq7nBHeY1rhhKdV6umNe','5BTPHpjJl7JewU2mwd3qCek716Fyz8Q3AgDhgJIYxQ2hO' )\\n\\napi = tweepy.API(auth)\\n\\nfollower_count = api.get_user('@andBeyondSafari').followers_count\\nprint(follower_count)\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This a code for getting the followers count of a certain user\n",
    "'''import tweepy\n",
    "\n",
    "auth = tweepy.OAuthHandler('hFHw04mGPdEztZe5CMVBTsq9l','cczG6tshnnIFZbdrSNIP7r22NzaFfUsKIC6cRdaglVfP5GyYaN' )\n",
    "auth.set_access_token('990262908696387584-HTgPRm2tySXaq7nBHeY1rhhKdV6umNe','5BTPHpjJl7JewU2mwd3qCek716Fyz8Q3AgDhgJIYxQ2hO' )\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "follower_count = api.get_user('@andBeyondSafari').followers_count\n",
    "print(follower_count)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import tweepy\\nimport codecs\\n\\nauth = tweepy.OAuthHandler(\\'hFHw04mGPdEztZe5CMVBTsq9l\\',\\'cczG6tshnnIFZbdrSNIP7r22NzaFfUsKIC6cRdaglVfP5GyYaN\\' )\\nauth.set_access_token(\\'990262908696387584-HTgPRm2tySXaq7nBHeY1rhhKdV6umNe\\',\\'5BTPHpjJl7JewU2mwd3qCek716Fyz8Q3AgDhgJIYxQ2hO\\' )\\n\\napi = tweepy.API(auth)\\nresults = api.user_timeline(screen_name=\"@Trevornoah\",count=100)\\nfor result in results:\\n    print(\\'Text: \\', ascii(result.text))\\n    print(\\'User: \\', ascii(result.user.screen_name))\\n    print(\\'# Retweets: \\', result.retweet_count.sum())\\n    print(\\'# Favorites: \\', result.favorite_count)'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using tweepy and codecs we can be able to get the tweets, retweet_count and favorite_count\n",
    "'''import tweepy\n",
    "import codecs\n",
    "\n",
    "auth = tweepy.OAuthHandler('hFHw04mGPdEztZe5CMVBTsq9l','cczG6tshnnIFZbdrSNIP7r22NzaFfUsKIC6cRdaglVfP5GyYaN' )\n",
    "auth.set_access_token('990262908696387584-HTgPRm2tySXaq7nBHeY1rhhKdV6umNe','5BTPHpjJl7JewU2mwd3qCek716Fyz8Q3AgDhgJIYxQ2hO' )\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "results = api.user_timeline(screen_name=\"@Trevornoah\",count=100)\n",
    "for result in results:\n",
    "    print('Text: ', ascii(result.text))\n",
    "    print('User: ', ascii(result.user.screen_name))\n",
    "    print('# Retweets: ', result.retweet_count.sum())\n",
    "    print('# Favorites: ', result.favorite_count)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also use this code segment that returns a csv file which has the tweets, retweet_count and favorite_count\n",
    "import tweepy\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "consumer_key = os.environ.get('consumer_key')\n",
    "consumer_secret = os.environ.get('consumer_secret')\n",
    "access_token = os.environ.get('access_token')\n",
    "access_token_secret = os.environ.get('access_token_secret')\n",
    "\n",
    "def get_tweets(screen_name):\n",
    "\n",
    "    auth = tweepy.OAuthHandler('hFHw04mGPdEztZe5CMVBTsq9l', 'cczG6tshnnIFZbdrSNIP7r22NzaFfUsKIC6cRdaglVfP5GyYaN')\n",
    "    auth.set_access_token('990262908696387584-HTgPRm2tySXaq7nBHeY1rhhKdV6umNe', '5BTPHpjJl7JewU2mwd3qCek716Fyz8Q3AgDhgJIYxQ2hO')\n",
    "    api = tweepy.API(auth)\n",
    "\n",
    "    #initialize a list to hold all the tweepy Tweets\n",
    "    alltweets = []  \n",
    "\n",
    "    #make initial request for most recent tweets (200 is the maximum allowed count)\n",
    "    new_tweets = api.user_timeline(screen_name = screen_name, count=200)\n",
    "\n",
    "    #save most recent tweets\n",
    "    alltweets.extend(new_tweets)\n",
    "\n",
    "    #save the id of the oldest tweet minus one\n",
    "    oldest = alltweets[-1].id - 1\n",
    "\n",
    "    #keep grabbing tweets until there are no tweets left to grab. \n",
    "    # Limit set to around 3k tweets, can be edited to preferred number.\n",
    "    while len(new_tweets) > 0:\n",
    "        print(\"getting tweets before %s\" % (oldest))\n",
    "\n",
    "        #all subsiquent requests use the max_id arg to prevent duplicates\n",
    "        new_tweets = api.user_timeline(screen_name = screen_name,count=200, max_id=oldest)\n",
    "\n",
    "        #save most recent tweets\n",
    "        alltweets.extend(new_tweets)\n",
    "\n",
    "        #update the id of the oldest tweet less one\n",
    "        oldest = alltweets[-1].id - 1\n",
    "\n",
    "        print(\"...%s tweets downloaded so far\" % (len(alltweets)))   \n",
    "\n",
    "    #transform the tweets into a 2D array that will populate the csv \n",
    "    outtweets = [[tweet.id_str, tweet.created_at,tweet.retweet_count,tweet.favorite_count, tweet.text.encode(\"utf-8\")] for tweet in alltweets]\n",
    "\n",
    "    #write the csv  \n",
    "    with open('%s_tweets.csv' % screen_name, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"id\",\"created_at\",\"retweet_count\",\"favorite_count\",\"text\"])\n",
    "        writer.writerows(outtweets)\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading our csv files into dataframes\n",
    "dataframe_1=pd.read_csv(\"Africaninfluencers - govt_influencers.csv\")\n",
    "dataframe_2=pd.read_csv(\"\\Desktop\\export_dataframe.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the the columns with twitter handles\n",
    "dataframe1=dataframe_1['Twitter_name']\n",
    "dataframe2=dataframe_2['Twittername']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the_100 = dataframe1.unique()\\nthe_leaders_response = dataframe2.unique()\\nl1 = the_100.astype(str).tolist() \\nl2 = the_leaders_response.astype(str).tolist()\\naccounts = l1 + l2\\ntry:\\n    if __name__ == '__main__':\\n    #loop through the handles in the list\\n        for i,name in enumerate(accounts):\\n              get_tweets(name)\\n            \\nexcept tweepy.error.TweepError:\\n    pass\\n      \""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fetch the unique handles from the top_100 and leaders dataframes\n",
    "# convert to list the merge them to be one list.\n",
    "#we also pass tweepy error \n",
    "'''the_100 = dataframe1.unique()\n",
    "the_leaders_response = dataframe2.unique()\n",
    "l1 = the_100.astype(str).tolist() \n",
    "l2 = the_leaders_response.astype(str).tolist()\n",
    "accounts = l1 + l2\n",
    "try:\n",
    "    if __name__ == '__main__':\n",
    "    #loop through the handles in the list\n",
    "        for i,name in enumerate(accounts):\n",
    "              get_tweets(name)\n",
    "            \n",
    "except tweepy.error.TweepError:\n",
    "    pass\n",
    "      '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aF8ORwSNCCaL"
   },
   "source": [
    "This is a python code(python classes and functions) that will illustrate and assist in searching and fetching data from twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WOd13zwajRVQ"
   },
   "outputs": [],
   "source": [
    "class tweetsearch():\n",
    "    '''\n",
    "    This is a basic class to search and download twitter data.\n",
    "    You can build up on it to extend the functionalities for more \n",
    "    sophisticated analysis\n",
    "    '''\n",
    "    def __init__(self, cols=None,auth=None):\n",
    "        #\n",
    "        if not cols is None:\n",
    "            self.cols = cols\n",
    "        else:\n",
    "            self.cols = ['id', 'created_at', 'source', 'original_text','clean_text', \n",
    "                    'sentiment','polarity','subjectivity', 'lang',\n",
    "                    'favorite_count', 'retweet_count','likes' 'original_author',   \n",
    "                    'possibly_sensitive', 'hashtags',\n",
    "                    'user_mentions', 'place', 'place_coord_boundaries']\n",
    "            \n",
    "        if auth is None:\n",
    "            \n",
    "            #Variables that contains the user credentials to access Twitter API \n",
    "            consumer_key = os.environ.get('consumer_key')\n",
    "            consumer_secret = os.environ.get('consumer_secret')\n",
    "            access_token = os.environ.get('access_token')\n",
    "            access_token_secret = os.environ.get('access_token_secret')\n",
    "            \n",
    "\n",
    "\n",
    "            #This handles Twitter authetification and the connection to Twitter Streaming API\n",
    "            auth = OAuthHandler('xxx', 'xxx')\n",
    "            auth.set_access_token('xxx', 'xxx')\n",
    "            \n",
    "\n",
    "        #            \n",
    "        self.auth = auth\n",
    "        self.api = tweepy.API(auth) \n",
    "        self.filtered_tweet = ''\n",
    "            \n",
    "\n",
    "    def clean_tweets(self, twitter_text):\n",
    "\n",
    "        #use pre processor\n",
    "        tweet = p.clean(twitter_text)\n",
    "\n",
    "         #HappyEmoticons\n",
    "        emoticons_happy = set([\n",
    "            ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "            ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "            '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "            'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "            '<3'\n",
    "            ])\n",
    "\n",
    "        # Sad Emoticons\n",
    "        emoticons_sad = set([\n",
    "            ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "            ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "            ':c', ':{', '>:\\\\', ';('\n",
    "            ])\n",
    "\n",
    "        #Emoji patterns\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                 u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                 u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                 u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                 u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                 u\"\\U00002702-\\U000027B0\"\n",
    "                 u\"\\U000024C2-\\U0001F251\"\n",
    "                 \"]+\", flags=re.UNICODE)\n",
    "\n",
    "        #combine sad and happy emoticons\n",
    "        emoticons = emoticons_happy.union(emoticons_sad)\n",
    "\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        word_tokens = nltk.word_tokenize(tweet)\n",
    "        #after tweepy preprocessing the colon symbol left remain after      \n",
    "        #removing mentions\n",
    "        tweet = re.sub(r':', '', tweet)\n",
    "        tweet = re.sub(r'â€šÃ„Â¶', '', tweet)\n",
    "\n",
    "        #replace consecutive non-ASCII characters with a space\n",
    "        tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    "\n",
    "        #remove emojis from tweet\n",
    "        tweet = emoji_pattern.sub(r'', tweet)\n",
    "\n",
    "        #filter using NLTK library append it to a string\n",
    "        filtered_tweet = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "        #looping through conditions\n",
    "        filtered_tweet = []    \n",
    "        for w in word_tokens:\n",
    "        #check tokens against stop words , emoticons and punctuations\n",
    "            if w not in stop_words and w not in emoticons and w not in string.punctuation:\n",
    "                filtered_tweet.append(w)\n",
    "\n",
    "        return ' '.join(filtered_tweet)            \n",
    "\n",
    "    def get_tweets(self, keyword, csvfile=None):\n",
    "        \n",
    "        \n",
    "        df = pd.DataFrame(columns=self.cols)\n",
    "        \n",
    "        if not csvfile is None:\n",
    "            #If the file exists, then read the existing data from the CSV file.\n",
    "            if os.path.exists(csvfile):\n",
    "                df = pd.read_csv(csvfile, header=0)\n",
    "            \n",
    "\n",
    "        #page attribute in tweepy.cursor and iteration\n",
    "        for page in tweepy.Cursor(self.api.search, q=keyword,count=500, include_rts=False).pages():\n",
    "\n",
    "            # the you receive from the Twitter API is in a JSON format and has quite an amount of information attached\n",
    "            for status in page:\n",
    "                \n",
    "                new_entry = []\n",
    "                status = status._json\n",
    "                \n",
    "                #filter by language\n",
    "                if status['lang'] != 'en':\n",
    "                    continue\n",
    "\n",
    "                \n",
    "                #if this tweet is a retweet update retweet count\n",
    "                if status['created_at'] in df['created_at'].values:\n",
    "                    i = df.loc[df['created_at'] == status['created_at']].index[0]\n",
    "                    #\n",
    "                    cond1 = status['favorite_count'] != df.at[i, 'favorite_count']\n",
    "                    cond2 = status['retweet_count'] != df.at[i, 'retweet_count']\n",
    "                    if cond1 or cond2:\n",
    "                        df.at[i, 'favorite_count'] = status['favorite_count']\n",
    "                        df.at[i, 'retweet_count'] = status['retweet_count']\n",
    "                    continue\n",
    "\n",
    "                #calculate sentiment\n",
    "                filtered_tweet = self.clean_tweets(status['text'])\n",
    "                blob = TextBlob(filtered_tweet)\n",
    "                Sentiment = blob.sentiment     \n",
    "                polarity = Sentiment.polarity\n",
    "                subjectivity = Sentiment.subjectivity\n",
    "\n",
    "                new_entry += [status['id'], status['created_at'],\n",
    "                              status['source'], status['text'], filtered_tweet, \n",
    "                              Sentiment,polarity,subjectivity, status['lang'],\n",
    "                              status['favorite_count'], status['retweet_count']]\n",
    "\n",
    "                new_entry.append(status['user']['screen_name'])\n",
    "\n",
    "                try:\n",
    "                    is_sensitive = status['possibly_sensitive']\n",
    "                except KeyError:\n",
    "                    is_sensitive = None\n",
    "\n",
    "                new_entry.append(is_sensitive)\n",
    "\n",
    "                hashtags = \", \".join([hashtag_item['text'] for hashtag_item in status['entities']['hashtags']])\n",
    "                new_entry.append(hashtags) #append the hashtags\n",
    "\n",
    "                #\n",
    "                mentions = \", \".join([mention['screen_name'] for mention in status['entities']['user_mentions']])\n",
    "                new_entry.append(mentions) #append the user mentions\n",
    "\n",
    "                try:\n",
    "                    xyz = status['place']['bounding_box']['coordinates']\n",
    "                    coordinates = [coord for loc in xyz for coord in loc]\n",
    "                except TypeError:\n",
    "                    coordinates = None\n",
    "                #\n",
    "                new_entry.append(coordinates)\n",
    "\n",
    "                try:\n",
    "                    location = status['user']['location']\n",
    "                except TypeError:\n",
    "                    location = ''\n",
    "                #\n",
    "                new_entry.append(location)\n",
    "\n",
    "                #now append a row to the dataframe\n",
    "                single_tweet_df = pd.DataFrame([new_entry], columns=self.cols)\n",
    "                df = df.append(single_tweet_df, ignore_index=True)\n",
    "\n",
    "        if not csvfile is None:\n",
    "            #save it to file\n",
    "            df.to_csv(csvfile, columns=self.cols, index=False, encoding=\"utf-8\")\n",
    "            \n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O2JtfhFBssaS"
   },
   "source": [
    "Now we search twitter and fetch data using the keywords and hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "10DnIPJojRVU",
    "outputId": "632d2d3a-fe31-486b-8360-4b4f6ddef629"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#get file if you have already downloaded what you wanted\n",
    "df = pd.read_csv(tweets_file, header=0)\n",
    "\n",
    "##get data on keywords\n",
    "ts = tweetsearch()\n",
    "df = ts.get_tweets('@News24')    #you saved the \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the sum of the columns of the returned data to find the total number of user_mentions\n",
    "df.sum(axis = 0, skipna = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''We will now the data of the top influencers from twitter;the screen_name,description,statuses_count,friends_count,\n",
    "followers_count,Account_age(in days),Average tweets per day, most mentioned twitter users and most 10 used hashtags.\n",
    "account_list1 is the list of the twitter handles of the top influencers.'''\n",
    "try:\n",
    "    if len(account_list1) > 0:\n",
    "        for target in account_list1:\n",
    "            print(\"Getting data for \" + target)\n",
    "            item =api.get_user(target)\n",
    "            print(\"name: \" + item.name)\n",
    "            print(\"screen_name: \" + item.screen_name)\n",
    "            print(\"description: \" + item.description)\n",
    "            print(\"statuses_count: \" + str(item.statuses_count))\n",
    "            print(\"friends_count: \" + str(item.friends_count))\n",
    "            print(\"followers_count: \" + str(item.followers_count))\n",
    "            tweets = item.statuses_count\n",
    "            account_created_date = item.created_at\n",
    "            delta = datetime.utcnow() - account_created_date\n",
    "            account_age_days = delta.days\n",
    "            print(\"Account age (in days): \" + str(account_age_days))\n",
    "            if account_age_days > 0:\n",
    "                print(\"Average tweets per day: \" + \"%.2f\"%(float(tweets)/float(account_age_days)))\n",
    "            hashtags = []\n",
    "            mentions = []\n",
    "            tweet_count = 0\n",
    "            end_date = datetime.utcnow() - timedelta(days=100)\n",
    "            for status in tweepy.Cursor(api.user_timeline,id=target).items():\n",
    "                tweet_count += 1\n",
    "                if hasattr(status, \"entities\"):\n",
    "                    entities = status.entities\n",
    "                    if \"hashtags\" in entities:\n",
    "                        for ent in entities[\"hashtags\"]:\n",
    "                            if ent is not None:\n",
    "                                if \"text\" in ent:\n",
    "                                    hashtag = ent[\"text\"]\n",
    "                                    if hashtag is not None:\n",
    "                                        hashtags.append(hashtag)\n",
    "                    if \"user_mentions\" in entities:\n",
    "                        for ent in entities[\"user_mentions\"]:\n",
    "                            if ent is not None:\n",
    "                                if \"screen_name\" in ent:\n",
    "                                    name = ent[\"screen_name\"]\n",
    "                                    if name is not None:\n",
    "                                        mentions.append(name)\n",
    "                if status.created_at < end_date:\n",
    "                    break\n",
    "            print\n",
    "            print(\"Most mentioned Twitter users:\")\n",
    "            for item, count in Counter(mentions).most_common(10):\n",
    "                print(item + \"\\t\" + str(count))\n",
    "\n",
    "            print\n",
    "            print(\"Most used hashtags:\")\n",
    "            for item, count in Counter(hashtags).most_common(10):\n",
    "                print(item + \"\\t\" + str(count))\n",
    "\n",
    "            print\n",
    "            print (\"All done. Processed \" + str(tweet_count) + \" tweets.\")\n",
    "            print\n",
    "        \n",
    "except tweepy.error.TweepError:\n",
    "    pass  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Twitter Data Mining Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
