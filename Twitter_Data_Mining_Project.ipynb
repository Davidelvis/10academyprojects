{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hMBdgi9K9Q6p"
   },
   "source": [
    "Natural Language Processing (NLP) is a unique subset of Machine Learning which cares about the real life unstructured data. Although computers cannot identify and process the string inputs, the libraries like NLTK, TextBlob and many others found a way to process string mathematically. Twitter is a platform where most of the people express their feelings towards the current context. In this Notebook i will search data; Covid-19 data mostly and then stream from twitter. Then i will filter the data and do some analysis on the data. I will use Twitter API get the data from twitter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ApGG-mnBBpQr"
   },
   "source": [
    "First we will import the libraries that we will be using in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UI2fIQFxrNLB",
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "\n",
    "# to view all columns\n",
    "pd.set_option(\"display.max.columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d8LdlCQHqv7Q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3pf5Xapqrq0M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in /Users/DClinton/opt/anaconda3/lib/python3.7/site-packages (3.8.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/DClinton/opt/anaconda3/lib/python3.7/site-packages (from tweepy) (1.3.0)\n",
      "Requirement already satisfied: requests>=2.11.1 in /Users/DClinton/opt/anaconda3/lib/python3.7/site-packages (from tweepy) (2.24.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/DClinton/opt/anaconda3/lib/python3.7/site-packages (from tweepy) (1.14.0)\n",
      "Requirement already satisfied: PySocks>=1.5.7 in /Users/DClinton/opt/anaconda3/lib/python3.7/site-packages (from tweepy) (1.7.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/DClinton/opt/anaconda3/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->tweepy) (3.1.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/DClinton/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.11.1->tweepy) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/DClinton/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.11.1->tweepy) (2019.11.28)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/DClinton/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.11.1->tweepy) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/DClinton/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.11.1->tweepy) (1.25.8)\n",
      "Requirement already satisfied: nltk in /Users/DClinton/opt/anaconda3/lib/python3.7/site-packages (3.4.5)\n",
      "Requirement already satisfied: six in /Users/DClinton/opt/anaconda3/lib/python3.7/site-packages (from nltk) (1.14.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/DClinton/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweet-preprocessor in /Users/DClinton/opt/anaconda3/lib/python3.7/site-packages (0.6.0)\r\n"
     ]
    }
   ],
   "source": [
    "#Import the necessary methods from tweepy library  \n",
    "\n",
    "#install tweepy if you don't have it\n",
    "!pip install tweepy\n",
    "import tweepy\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "\n",
    "#sentiment analysis package\n",
    "#!pip install textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "#general text pre-processor\n",
    "!pip install nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "\n",
    "#tweet pre-processor \n",
    "!pip install tweet-preprocessor\n",
    "import preprocessor as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Name</th>\n",
       "      <th>Twittername</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>Jeffrey Gettleman</td>\n",
       "      <td>@gettleman)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>99</td>\n",
       "      <td>Africa24 Media</td>\n",
       "      <td>@a24media)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98</td>\n",
       "      <td>Scapegoat</td>\n",
       "      <td>@andiMakinana)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>97</td>\n",
       "      <td>Africa Check</td>\n",
       "      <td>@AfricaCheck)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>96</td>\n",
       "      <td>James Copnall</td>\n",
       "      <td>@JamesCopnall)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5</td>\n",
       "      <td>Julius Sello Malema</td>\n",
       "      <td>@Julius_S_Malema)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>4</td>\n",
       "      <td>News24</td>\n",
       "      <td>@News24)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>3</td>\n",
       "      <td>Jacob G. Zuma</td>\n",
       "      <td>@SAPresident)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2</td>\n",
       "      <td>Gareth Cliff</td>\n",
       "      <td>@GarethCliff)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "      <td>Trevor Noah</td>\n",
       "      <td>@Trevornoah)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Rank                   Name        Twittername\n",
       "0    100     Jeffrey Gettleman         @gettleman)\n",
       "1     99        Africa24 Media          @a24media)\n",
       "2     98             Scapegoat      @andiMakinana)\n",
       "3     97          Africa Check       @AfricaCheck)\n",
       "4     96         James Copnall      @JamesCopnall)\n",
       "..   ...                    ...                ...\n",
       "95     5   Julius Sello Malema   @Julius_S_Malema)\n",
       "96     4                News24            @News24)\n",
       "97     3         Jacob G. Zuma       @SAPresident)\n",
       "98     2          Gareth Cliff       @GarethCliff)\n",
       "99     1           Trevor Noah        @Trevornoah)\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('\\Desktop\\export_dataframe.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#striping off the last character in the twittername column\n",
    "data['Twittername'] = data['Twittername'].str.rstrip(')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Name</th>\n",
       "      <th>Twittername</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>Jeffrey Gettleman</td>\n",
       "      <td>@gettleman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>99</td>\n",
       "      <td>Africa24 Media</td>\n",
       "      <td>@a24media</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98</td>\n",
       "      <td>Scapegoat</td>\n",
       "      <td>@andiMakinana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>97</td>\n",
       "      <td>Africa Check</td>\n",
       "      <td>@AfricaCheck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>96</td>\n",
       "      <td>James Copnall</td>\n",
       "      <td>@JamesCopnall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5</td>\n",
       "      <td>Julius Sello Malema</td>\n",
       "      <td>@Julius_S_Malema</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>4</td>\n",
       "      <td>News24</td>\n",
       "      <td>@News24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>3</td>\n",
       "      <td>Jacob G. Zuma</td>\n",
       "      <td>@SAPresident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2</td>\n",
       "      <td>Gareth Cliff</td>\n",
       "      <td>@GarethCliff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "      <td>Trevor Noah</td>\n",
       "      <td>@Trevornoah</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Rank                   Name       Twittername\n",
       "0    100     Jeffrey Gettleman         @gettleman\n",
       "1     99        Africa24 Media          @a24media\n",
       "2     98             Scapegoat      @andiMakinana\n",
       "3     97          Africa Check       @AfricaCheck\n",
       "4     96         James Copnall      @JamesCopnall\n",
       "..   ...                    ...               ...\n",
       "95     5   Julius Sello Malema   @Julius_S_Malema\n",
       "96     4                News24            @News24\n",
       "97     3         Jacob G. Zuma       @SAPresident\n",
       "98     2          Gareth Cliff       @GarethCliff\n",
       "99     1           Trevor Noah        @Trevornoah\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           @gettleman\n",
       "1            @a24media\n",
       "2        @andiMakinana\n",
       "3         @AfricaCheck\n",
       "4        @JamesCopnall\n",
       "            ...       \n",
       "95    @Julius_S_Malema\n",
       "96             @News24\n",
       "97        @SAPresident\n",
       "98        @GarethCliff\n",
       "99         @Trevornoah\n",
       "Name: Twittername, Length: 100, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1=data['Twittername']\n",
    "list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing twittername data into atext file\n",
    "with open('your_file.txt', 'w') as f:\n",
    "    for item in list1:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom __future__ import unicode_literals\\nimport tweepy\\nimport time\\nimport unicodecsv as csv\\nimport codecs\\nfrom importlib import reload\\nimport sys \\nwith open('your_file.txt', 'r') as targets_file:\\n    targets_list = targets_file.readlines()\\n\\ntargets_list_filtered = filter(None, targets_list[0].split('\\r'))\\n\\ntargets_list_cleaned = [] \\n\\nfor item in targets_list_filtered:\\n    targets_list_cleaned.append(item.strip('\\n'))\\n\\nwith codecs.open('output.csv', 'w', 'utf-8') as outcsv:\\n    outfile = csv.DictWriter(outcsv, fieldnames=['uID', 'Username', 'Follower Count', 'Verified'])\\n    outfile.writeheader()\\n\\n    for idx, target in enumerate(targets_list_cleaned):\\n        try:\\n            user = api.get_user(target)\\n            outfile.writerow({'uID': target, 'Username': user.name, 'Follower Count': user.followers_count, 'Verified': user.verified})\\n            print(idx, target, user.name, user.followers_count, user.verified)\\n        except tweepy.TweepError as e:\\n            # outfile.writerow(e.message)\\n            print(idx, target, e.message)\\n        \""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from __future__ import unicode_literals\n",
    "import tweepy\n",
    "import time\n",
    "import unicodecsv as csv\n",
    "import codecs\n",
    "from importlib import reload\n",
    "import sys \n",
    "with open('your_file.txt', 'r') as targets_file:\n",
    "    targets_list = targets_file.readlines()\n",
    "\n",
    "targets_list_filtered = filter(None, targets_list[0].split('\\r'))\n",
    "\n",
    "targets_list_cleaned = [] \n",
    "\n",
    "for item in targets_list_filtered:\n",
    "    targets_list_cleaned.append(item.strip('\\n'))\n",
    "\n",
    "with codecs.open('output.csv', 'w', 'utf-8') as outcsv:\n",
    "    outfile = csv.DictWriter(outcsv, fieldnames=['uID', 'Username', 'Follower Count', 'Verified'])\n",
    "    outfile.writeheader()\n",
    "\n",
    "    for idx, target in enumerate(targets_list_cleaned):\n",
    "        try:\n",
    "            user = api.get_user(target)\n",
    "            outfile.writerow({'uID': target, 'Username': user.name, 'Follower Count': user.followers_count, 'Verified': user.verified})\n",
    "            print(idx, target, user.name, user.followers_count, user.verified)\n",
    "        except tweepy.TweepError as e:\n",
    "            # outfile.writerow(e.message)\n",
    "            print(idx, target, e.message)\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25699\n"
     ]
    }
   ],
   "source": [
    "#This a code for getting the followers count of a certain user\n",
    "import tweepy\n",
    "\n",
    "auth = tweepy.OAuthHandler('hFHw04mGPdEztZe5CMVBTsq9l','cczG6tshnnIFZbdrSNIP7r22NzaFfUsKIC6cRdaglVfP5GyYaN' )\n",
    "auth.set_access_token('990262908696387584-HTgPRm2tySXaq7nBHeY1rhhKdV6umNe','5BTPHpjJl7JewU2mwd3qCek716Fyz8Q3AgDhgJIYxQ2hO' )\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "follower_count = api.get_user('@gettleman').followers_count\n",
    "print(follower_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using tweepy and codecs we can be able to get the tweets, retweet_count and favorite_count\n",
    "import tweepy\n",
    "import codecs\n",
    "\n",
    "auth = tweepy.OAuthHandler('hFHw04mGPdEztZe5CMVBTsq9l','cczG6tshnnIFZbdrSNIP7r22NzaFfUsKIC6cRdaglVfP5GyYaN' )\n",
    "auth.set_access_token('990262908696387584-HTgPRm2tySXaq7nBHeY1rhhKdV6umNe','5BTPHpjJl7JewU2mwd3qCek716Fyz8Q3AgDhgJIYxQ2hO' )\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "results = api.user_timeline(screen_name=\"@Trevornoah\",count=100)\n",
    "for result in results:\n",
    "    print('Text: ', ascii(result.text))\n",
    "    print('User: ', ascii(result.user.screen_name))\n",
    "    print('# Retweets: ', result.retweet_count)\n",
    "    print('# Favorites: ', result.favorite_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also use this code segment that returns a csv file which has the tweets, retweet_count and favorite_count\n",
    "import tweepy\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "consumer_key = os.environ.get('consumer_key')\n",
    "consumer_secret = os.environ.get('consumer_secret')\n",
    "access_token = os.environ.get('access_token')\n",
    "access_token_secret = os.environ.get('access_token_secret')\n",
    "\n",
    "def get_tweets(screen_name):\n",
    "\n",
    "    auth = tweepy.OAuthHandler('hFHw04mGPdEztZe5CMVBTsq9l', 'cczG6tshnnIFZbdrSNIP7r22NzaFfUsKIC6cRdaglVfP5GyYaN')\n",
    "    auth.set_access_token('990262908696387584-HTgPRm2tySXaq7nBHeY1rhhKdV6umNe', '5BTPHpjJl7JewU2mwd3qCek716Fyz8Q3AgDhgJIYxQ2hO')\n",
    "    api = tweepy.API(auth)\n",
    "\n",
    "    #initialize a list to hold all the tweepy Tweets\n",
    "    alltweets = []  \n",
    "\n",
    "    #make initial request for most recent tweets (200 is the maximum allowed count)\n",
    "    new_tweets = api.user_timeline(screen_name = screen_name, count=200)\n",
    "\n",
    "    #save most recent tweets\n",
    "    alltweets.extend(new_tweets)\n",
    "\n",
    "    #save the id of the oldest tweet minus one\n",
    "    oldest = alltweets[-1].id - 1\n",
    "\n",
    "    #keep grabbing tweets until there are no tweets left to grab. \n",
    "    # Limit set to around 3k tweets, can be edited to preferred number.\n",
    "    while len(new_tweets) > 0:\n",
    "        print(\"getting tweets before %s\" % (oldest))\n",
    "\n",
    "        #all subsiquent requests use the max_id arg to prevent duplicates\n",
    "        new_tweets = api.user_timeline(screen_name = screen_name,count=200, max_id=oldest)\n",
    "\n",
    "        #save most recent tweets\n",
    "        alltweets.extend(new_tweets)\n",
    "\n",
    "        #update the id of the oldest tweet less one\n",
    "        oldest = alltweets[-1].id - 1\n",
    "\n",
    "        print(\"...%s tweets downloaded so far\" % (len(alltweets)))   \n",
    "\n",
    "    #transform the tweets into a 2D array that will populate the csv \n",
    "    outtweets = [[tweet.id_str, tweet.created_at,tweet.retweet_count,tweet.favorite_count, tweet.text.encode(\"utf-8\")] for tweet in alltweets]\n",
    "\n",
    "    #write the csv  \n",
    "    with open('%s_tweets.csv' % screen_name, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"id\",\"created_at\",\"retweet_count\",\"favorite_count\",\"text\"])\n",
    "        writer.writerows(outtweets)\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading our csv files into dataframes\n",
    "dataframe_1=pd.read_csv(\"Africaninfluencers - govt_influencers.csv\")\n",
    "dataframe_2=pd.read_csv(\"\\Desktop\\export_dataframe.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the the columns with twitter handles\n",
    "dataframe1=dataframe_1['Twitter_name']\n",
    "dataframe2=dataframe_2['Twittername']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the unique handles from the top_100 and leaders dataframes\n",
    "# convert to list the merge them to be one list.\n",
    "#we also pass tweepy error \n",
    "the_100 = dataframe1.unique()\n",
    "the_leaders_response = dataframe2.unique()\n",
    "l1 = the_100.astype(str).tolist() \n",
    "l2 = the_leaders_response.astype(str).tolist()\n",
    "accounts = l1 + l2\n",
    "try:\n",
    "    if __name__ == '__main__':\n",
    "    #loop through the handles in the list\n",
    "        for i,name in enumerate(accounts):\n",
    "              get_tweets(name)\n",
    "            \n",
    "except tweepy.error.TweepError:\n",
    "    pass\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aF8ORwSNCCaL"
   },
   "source": [
    "This is a python code(python classes and functions) that will illustrate and assist in searching and fetching data from twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WOd13zwajRVQ"
   },
   "outputs": [],
   "source": [
    "class tweetsearch():\n",
    "    '''\n",
    "    This is a basic class to search and download twitter data.\n",
    "    You can build up on it to extend the functionalities for more \n",
    "    sophisticated analysis\n",
    "    '''\n",
    "    def __init__(self, cols=None,auth=None):\n",
    "        #\n",
    "        if not cols is None:\n",
    "            self.cols = cols\n",
    "        else:\n",
    "            self.cols = ['id', 'created_at', 'source', 'original_text','clean_text', \n",
    "                    'sentiment','polarity','subjectivity', 'lang',\n",
    "                    'favorite_count', 'retweet_count','likes' 'original_author',   \n",
    "                    'possibly_sensitive', 'hashtags',\n",
    "                    'user_mentions', 'place', 'place_coord_boundaries']\n",
    "            \n",
    "        if auth is None:\n",
    "            '''\n",
    "            #Variables that contains the user credentials to access Twitter API \n",
    "            consumer_key = os.environ.get('2fGZDlTA6gcS0aqc3Vs9RkUsx')\n",
    "            consumer_secret = os.environ.get('KFFI3FcgY90NKtG1IReTWj4sQ77Y9osnFK0OoX9fUVuz44iEry')\n",
    "            access_token = os.environ.get('990262908696387584-HTgPRm2tySXaq7nBHeY1rhhKdV6umNe')\n",
    "            access_token_secret = os.environ.get('5BTPHpjJl7JewU2mwd3qCek716Fyz8Q3AgDhgJIYxQ2hO')\n",
    "            '''\n",
    "\n",
    "\n",
    "            #This handles Twitter authetification and the connection to Twitter Streaming API\n",
    "            auth = OAuthHandler('2fGZDlTA6gcS0aqc3Vs9RkUsx', 'KFFI3FcgY90NKtG1IReTWj4sQ77Y9osnFK0OoX9fUVuz44iEry')\n",
    "            auth.set_access_token('990262908696387584-HTgPRm2tySXaq7nBHeY1rhhKdV6umNe', '5BTPHpjJl7JewU2mwd3qCek716Fyz8Q3AgDhgJIYxQ2hO')\n",
    "            \n",
    "\n",
    "        #            \n",
    "        self.auth = auth\n",
    "        self.api = tweepy.API(auth) \n",
    "        self.filtered_tweet = ''\n",
    "            \n",
    "\n",
    "    def clean_tweets(self, twitter_text):\n",
    "\n",
    "        #use pre processor\n",
    "        tweet = p.clean(twitter_text)\n",
    "\n",
    "         #HappyEmoticons\n",
    "        emoticons_happy = set([\n",
    "            ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "            ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "            '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "            'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "            '<3'\n",
    "            ])\n",
    "\n",
    "        # Sad Emoticons\n",
    "        emoticons_sad = set([\n",
    "            ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "            ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "            ':c', ':{', '>:\\\\', ';('\n",
    "            ])\n",
    "\n",
    "        #Emoji patterns\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                 u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                 u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                 u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                 u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                 u\"\\U00002702-\\U000027B0\"\n",
    "                 u\"\\U000024C2-\\U0001F251\"\n",
    "                 \"]+\", flags=re.UNICODE)\n",
    "\n",
    "        #combine sad and happy emoticons\n",
    "        emoticons = emoticons_happy.union(emoticons_sad)\n",
    "\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        word_tokens = nltk.word_tokenize(tweet)\n",
    "        #after tweepy preprocessing the colon symbol left remain after      \n",
    "        #removing mentions\n",
    "        tweet = re.sub(r':', '', tweet)\n",
    "        tweet = re.sub(r'â€šÃ„Â¶', '', tweet)\n",
    "\n",
    "        #replace consecutive non-ASCII characters with a space\n",
    "        tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    "\n",
    "        #remove emojis from tweet\n",
    "        tweet = emoji_pattern.sub(r'', tweet)\n",
    "\n",
    "        #filter using NLTK library append it to a string\n",
    "        filtered_tweet = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "        #looping through conditions\n",
    "        filtered_tweet = []    \n",
    "        for w in word_tokens:\n",
    "        #check tokens against stop words , emoticons and punctuations\n",
    "            if w not in stop_words and w not in emoticons and w not in string.punctuation:\n",
    "                filtered_tweet.append(w)\n",
    "\n",
    "        return ' '.join(filtered_tweet)            \n",
    "\n",
    "    def get_tweets(self, keyword, csvfile=None):\n",
    "        \n",
    "        \n",
    "        df = pd.DataFrame(columns=self.cols)\n",
    "        \n",
    "        if not csvfile is None:\n",
    "            #If the file exists, then read the existing data from the CSV file.\n",
    "            if os.path.exists(csvfile):\n",
    "                df = pd.read_csv(csvfile, header=0)\n",
    "            \n",
    "\n",
    "        #page attribute in tweepy.cursor and iteration\n",
    "        for page in tweepy.Cursor(self.api.search, q=keyword,count=500, include_rts=False).pages():\n",
    "\n",
    "            # the you receive from the Twitter API is in a JSON format and has quite an amount of information attached\n",
    "            for status in page:\n",
    "                \n",
    "                new_entry = []\n",
    "                status = status._json\n",
    "                \n",
    "                #filter by language\n",
    "                if status['lang'] != 'en':\n",
    "                    continue\n",
    "\n",
    "                \n",
    "                #if this tweet is a retweet update retweet count\n",
    "                if status['created_at'] in df['created_at'].values:\n",
    "                    i = df.loc[df['created_at'] == status['created_at']].index[0]\n",
    "                    #\n",
    "                    cond1 = status['favorite_count'] != df.at[i, 'favorite_count']\n",
    "                    cond2 = status['retweet_count'] != df.at[i, 'retweet_count']\n",
    "                    if cond1 or cond2:\n",
    "                        df.at[i, 'favorite_count'] = status['favorite_count']\n",
    "                        df.at[i, 'retweet_count'] = status['retweet_count']\n",
    "                    continue\n",
    "\n",
    "                #calculate sentiment\n",
    "                filtered_tweet = self.clean_tweets(status['text'])\n",
    "                blob = TextBlob(filtered_tweet)\n",
    "                Sentiment = blob.sentiment     \n",
    "                polarity = Sentiment.polarity\n",
    "                subjectivity = Sentiment.subjectivity\n",
    "\n",
    "                new_entry += [status['id'], status['created_at'],\n",
    "                              status['source'], status['text'], filtered_tweet, \n",
    "                              Sentiment,polarity,subjectivity, status['lang'],\n",
    "                              status['favorite_count'], status['retweet_count']]\n",
    "\n",
    "                new_entry.append(status['user']['screen_name'])\n",
    "\n",
    "                try:\n",
    "                    is_sensitive = status['possibly_sensitive']\n",
    "                except KeyError:\n",
    "                    is_sensitive = None\n",
    "\n",
    "                new_entry.append(is_sensitive)\n",
    "\n",
    "                hashtags = \", \".join([hashtag_item['text'] for hashtag_item in status['entities']['hashtags']])\n",
    "                new_entry.append(hashtags) #append the hashtags\n",
    "\n",
    "                #\n",
    "                mentions = \", \".join([mention['screen_name'] for mention in status['entities']['user_mentions']])\n",
    "                new_entry.append(mentions) #append the user mentions\n",
    "\n",
    "                try:\n",
    "                    xyz = status['place']['bounding_box']['coordinates']\n",
    "                    coordinates = [coord for loc in xyz for coord in loc]\n",
    "                except TypeError:\n",
    "                    coordinates = None\n",
    "                #\n",
    "                new_entry.append(coordinates)\n",
    "\n",
    "                try:\n",
    "                    location = status['user']['location']\n",
    "                except TypeError:\n",
    "                    location = ''\n",
    "                #\n",
    "                new_entry.append(location)\n",
    "\n",
    "                #now append a row to the dataframe\n",
    "                single_tweet_df = pd.DataFrame([new_entry], columns=self.cols)\n",
    "                df = df.append(single_tweet_df, ignore_index=True)\n",
    "\n",
    "        if not csvfile is None:\n",
    "            #save it to file\n",
    "            df.to_csv(csvfile, columns=self.cols, index=False, encoding=\"utf-8\")\n",
    "            \n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O2JtfhFBssaS"
   },
   "source": [
    "Now we search twitter and fetch data using the keywords and hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c764RMVtsxlQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "10DnIPJojRVU",
    "outputId": "632d2d3a-fe31-486b-8360-4b4f6ddef629"
   },
   "outputs": [],
   "source": [
    "#covid_keywords ='covid-19kenya'  #hashtag based search\n",
    "#tweets_file = 'data/kenya_covid19_12july2020.json'\n",
    "\n",
    "#get file if you have already downloaded what you wanted\n",
    "#df = pd.read_csv(tweets_file, header=0)\n",
    "\n",
    "##get data on keywords\n",
    "ts = tweetsearch()\n",
    "df = ts.get_tweets('@_davidelvis')    #you saved the \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the sum of the columns of the returned data to find the total number of user_mentions\n",
    "df.sum(axis = 0, skipna = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fJ6YRre6qlDH"
   },
   "source": [
    "**Stream data and save it to file**\n",
    "\n",
    "In the above we saw how to search and fetch data, below we will see how we will stream data from twitter. Make sure you understand the difference between search and stream features of twitter api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wGL-p2V_qwz-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r6lcy009rX_e"
   },
   "outputs": [],
   "source": [
    "#This is a basic listener that writes received tweets to file.\n",
    "'''class StdOutListener(StreamListener):\n",
    "\n",
    "    def __init__(self,fhandle, stop_at = 1000):\n",
    "        self.tweet_counter = 0\n",
    "        self.stop_at = stop_at\n",
    "        self.fhandle = fhandle\n",
    "         \n",
    "        \n",
    "    def on_data(self, data):\n",
    "        self.fhandle.write(data)\n",
    "        \n",
    "        #stop if enough tweets are obtained\n",
    "        self.tweet_counter += 1   \n",
    "        if self.tweet_counter < self.stop_at:        \n",
    "            return True\n",
    "        else:\n",
    "            print('Max number of tweets reached: #tweets = ' + str(self.tweet_counter))\n",
    "            return False\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print (status)\n",
    "\n",
    "def stream_tweet_data(filename='df',\n",
    "                      keywords=['COVID19KE'],\n",
    "                      is_async=True):\n",
    "    # tweet topics to use as a filter. The tweets downloaded\n",
    "    # will have one of the topics in their text or hashtag \n",
    "\n",
    "    print('saving data to file: ',filename)\n",
    "\n",
    "    #print the tweet topics \n",
    "    print('Tweet Keywords are: ',keywords)\n",
    "    print('For testing case, please interupt the downloading process \\\n",
    "            using ctrl+x after about 5 mins ')\n",
    "    print('To keep streaming in the background, pass is_async=True')\n",
    "    '''\n",
    "    #Variables that contains the user credentials to access Twitter API \n",
    "    consumer_key = os.environ.get('TWITTER_API_KEY')\n",
    "    consumer_secret = os.environ.get('TWITTER_API_SECRET')\n",
    "    access_token = os.environ.get('TWITTER_ACCESS_TOKEN')\n",
    "    access_token_secret = os.environ.get('TWITTER_ACCESS_TOKEN_SECRET')\n",
    "    '''    \n",
    "    #open file\n",
    "    fhandle=open(filename,'w')\n",
    "\n",
    "    #This handles Twitter authetification and the connection to Twitter Streaming API\n",
    "    l = StdOutListener(fhandle)\n",
    "    auth = OAuthHandler('2fGZDlTA6gcS0aqc3Vs9RkUsx','KFFI3FcgY90NKtG1IReTWj4sQ77Y9osnFK0OoX9fUVuz44iEry' )\n",
    "    auth.set_access_token('990262908696387584-HTgPRm2tySXaq7nBHeY1rhhKdV6umNe','5BTPHpjJl7JewU2mwd3qCek716Fyz8Q3AgDhgJIYxQ2hO' )\n",
    "\n",
    "    stream = Stream(auth, l)\n",
    "\n",
    "    #This line filter Twitter Streams to capture data by the keywords: first argument to this code\n",
    "    stream.filter(track=keywords)\n",
    "\n",
    "    return None\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VLn2eylurCmq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "F8tcPcSMrNLL",
    "outputId": "18396f91-7cf6-46f5-e92a-29fadf36790c"
   },
   "outputs": [],
   "source": [
    "#calling the function for streaming data\n",
    "#stream_tweet_data(keywords=['@gettleman'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AuURVTMwx-Ou"
   },
   "outputs": [],
   "source": [
    "#tweets_file='df'\n",
    "#tweets_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SmHRJI_tjRVi",
    "outputId": "3c0b7a72-61ae-4b7e-84ea-04d212528596"
   },
   "outputs": [],
   "source": [
    "tweets_data = []\n",
    "for line in open(tweets_file, \"r\"):\n",
    "    try:\n",
    "        tweet = json.loads(line)\n",
    "        x=tweet['text']\n",
    "        tweets_data.append(tweet)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "\n",
    "print('saved numbers of tweets: ', len(tweets_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jxWa3Xq0ySZd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hlFKyGnYrNLX"
   },
   "outputs": [],
   "source": [
    "tweets = pd.DataFrame(columns=['text','lang','country'])\n",
    "\n",
    "tweets['text'] = list(map(lambda tweet: tweet['text'], tweets_data))\n",
    "tweets['lang'] = list(map(lambda tweet: tweet['lang'], tweets_data))\n",
    "tweets['country'] = list(map(lambda tweet: tweet['place']['country'] \\\n",
    "                             if tweet['place'] != None else None, \n",
    "                             tweets_data))\n",
    "\n",
    "\n",
    "tweets_by_lang = tweets['lang'].value_counts()\n",
    "tweets_by_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cfU3wQinjRVo"
   },
   "outputs": [],
   "source": [
    "tweets_by_country = tweets['country'].value_counts()\n",
    "tweets_by_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ETcUa_wQys6o"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 751
    },
    "colab_type": "code",
    "id": "aEPPoCBtrNLd",
    "outputId": "5e7e34d4-5f11-4d66-ac01-c110b39424c5"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.tick_params(axis='x', labelsize=15)\n",
    "ax.tick_params(axis='y', labelsize=10)\n",
    "ax.set_xlabel('Languages', fontsize=15)\n",
    "ax.set_ylabel('Number of tweets' , fontsize=15)\n",
    "ax.set_title('Top 5 languages and the number of tweets', fontsize=15, fontweight='bold')\n",
    "tweets_by_lang[:5].plot(ax=ax, kind='bar', color='red', rot=0)\n",
    "\n",
    "tweets_by_country = tweets['country'].value_counts()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.tick_params(axis='x', labelsize=15)\n",
    "ax.tick_params(axis='y', labelsize=10)\n",
    "ax.set_xlabel('Countries', fontsize=15)\n",
    "ax.set_ylabel('Number of tweets' , fontsize=15)\n",
    "ax.set_title('Top 5 countries', fontsize=15, fontweight='bold')\n",
    "tweets_by_country[:5].plot(ax=ax, kind='bar', color='blue', rot=45,)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Twitter Data Mining Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
